Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework from Controlled Languages to Robust Pipelines

Abstract syntax is an interlingual representation used in compilers. Grammatical Framework (GF) applies the abstract syntax idea to natural languages. The development of GF started in 1998, first as a tool for controlled language implementations, where it has gained an established position in both academic and commercial projects. GF provides grammar resources for over 40 languages, enabling accurate generation and translation, as well as grammar engineering
ools and components for mobile and Web applications. On the research side, the focus in the last ten years has been on scaling up GF to wide-coverage language processing. The concept of abstract syntax offers a unified view on many other approaches: Universal Dependencies, WordNets, FrameNets, Construction Grammars, and Abstract Meaning Representations. This makes it possible for GF to utilize data from the other approaches and to build robust pipelines. In return, GF can contribute to data-driven approaches by methods to transfer resources from one language to others, to augment data by rule-based generation, to check the consistency of hand- annotated corpora, and to pipe analyses into high-precision semantic back ends. This article gives an overview of the use of abstract syntax as interlingua through both established and emerging NLP applications involving GF.
1. Introduction
Like many other computing tasks, Natural Language Processing (NLP) can be divided to two phases: analysis and synthesis. The analysis phase involves Natural Language Understanding (NLU), whereas the synthesis phase may serve different kinds of tasks, such as Natural Language Generation (NLG), question answering, and conversion to logical forms. Machine Translation (MT) is a representative example of NLP, which can be structured as a combination of NLU from one language with NLG to another language. This architecture is shown in the Vauquois triangle (Figure 1, adapted from Vauquois 1968): The result of NLU is a meaning representation in an interlingua, and NLG renders this meaning faithfully in the target language.
For Vauquois, interlingua-based translation was the ideal, because it guarantees that the source language meaning is rendered faithfully in the target language, what- ever differences there are in the surface syntax. At the same time, he was aware that this method might not be realistic to implement in practice, and that approximations might be needed. Thus his triangle includes shortcuts on different levels: word-to-word transfer and syntactic transfer. Character-to-character transfer is a later approach used in Neural Machine Translation (NMT; Lee, Cho, and Hofmann 2016).
We have annotated Figure 1 with labels indicating how problematic each phase is. On the lowest level, lexical (morphological) analysis is well understood thanks to a long tradition and reliable analyzers; exceptions are provided by languages that do not mark word boundaries and by systems that have to recover from spelling mistakes. Going one step higher, syntactic parsing, which we take to include part-of- speech disambiguation, is uncertain, because natural languages are ambiguous and also because syntactic grammars are usually incomplete. For similar but even stronger reasons, semantic interpretation can be labeled as highly uncertain. Finally, the semantic interlingua itself is problematic, and all known attempts to build one have remained incomplete.
Target generation is in principle a solved problem if it starts from a formalized interlingua, since it can be performed by a deterministic program. But the transfer from source to target is less certain on the lower levels than on the higher levels; for example, on the word-to-word level, the order of words in the target language is just a guess.
Most traditional MT methods operate on the lower levels: Statistical MT (SMT; Koehn 2010) is usually either word-to-word or phrase-to-phrase transfer,1 and so are rule-based systems such as Apertium (Forcada et al. 2011). More recently, NMT has introduced character-to-character transfer, which Vauquois did not consider at all (Lee, Cho, and Hofmann 2016) . But in a system where all steps are uncertain, ranking the alternatives globally rather than separately in a pipeline actually does make sense, even though it comes with the price of not being able to use linguistic knowledge and to explain step by step how the translation is achieved.
At the same time as going down to the lowest possible level of transfer, NMT has made claims of using an interlingua, just not of the kind that humans construct but something that arises as an internal representation in a neural network (Lu et al. 2018). The main advantage is the same as in traditional interlinguas: one does not need to build n(n − 1) translation functions to cover all pairs of n languages, but it is enough to have 2n (Figure 2). In the NMT world, this method has been given the name zero- shot translation—translation between language pairs for which the system has not been explicitly trained. Similar advantages can be shown in other NLP tasks: For instance, the answer function in question answering can be defined uniformly in terms of the interlingua, rather than for all languages separately (Zimina et al. 2018).
A universal interlingua that accurately expresses all meaning in natural languages would be the ideal translation method. But no one has managed to build one, and it is not sure if the concept even makes sense. Therefore it is useful to consider interlinguas with less ambitious specifications:
• deep interlinguas, covering fragments of language with high precision,
• shallow interlinguas, with wide coverage but lower precision, and
• layered interlinguas, using shallow interlinguas as backup for deep ones.
Examples of the first kind can be found in Controlled Natural Languages (CNL), where the interlingua can be a domain-specific semantic structure or an “ontology”; see Kuhn (2014) for a recent survey of CNL, and Hutchins and Somers (1992) for its potential in translation. A deep interlingua can actually start with a semantic model and generate a fragment of language that matches it, rather than starting with a language and trying to find a semantic model.
Shallow interlinguas are representations of some aspects of language that help translation, without going as deep as the full meaning to be rendered. A simple example of a shallow interlingual representation would be a sequence of word senses, such as the ones in multilingual WordNets. The translator would first map source language words to these senses, and then map the senses as target language words in the same order. The words would often appear in wrong senses, in wrong order, or in wrong forms, but the translation could still be helpful.
A bit less shallow interlingua could add syntactic structure to the words, so that it could reorder the words or even enforce agreement in accordance with the rules of the target language. Such a system would still not always select correct word senses, identify multiword constructions, or change the syntactic structure when needed (Dorr 1994). But it could guarantee the grammatical correctness of the output.
Figure 3 shows the architecture of a layered interlingua system, where the transfer- based shortcuts of the original Vauquois triangle are replaced by interlinguas of varying depth. Such a system enjoys some of the advantages of the original interlingua idea—in particular, the linear scale-up as languages are added.
The main topic of this article is to show how to build and combine interlinguas of different levels, by using Grammatical Framework (GF; Ranta 2004b, 2011a), a tool designed for this very purpose. More precisely, GF is a special-purpose programming language for defining interlinguas (abstract syntaxes) and reversible mappings from them to individual languages (concrete syntaxes). GF has no single interlingua or fixed set of languages, but a set of tools for building new ones and reusing old ones.
GF was originally intended as a tool for CNL implementations, and it has been widely used in both academic and commercial CNL projects. But many research efforts in the last ten years have been devoted to scaling up beyond CNL. This has often happened by combining GF grammars with other approaches.
This article is an overview of the interlingua idea as implemented in GF, with focus on recent wide-coverage work. 
The Vauquois triangle, with transfer shortcuts replaced by interlinguas on various levels.
• Section 2 outlines the background of GF and its development from CNL to open domain tasks.
• Section 3 gives a quick tutorial on GF, with a focus on how linguistic variation can be modeled in terms of abstract and concrete syntax.
• Section 4 gives a summary of GF’s Resource Grammar Library (RGL), which aims to formalize the main syntactic structures of the world’s languages and plays a major role in both CNL and wide-coverage applications.
• Section 5 shows how NLP systems can be built by combining interlinguas of different levels, ranging from chunks of words to logical formulas.
• Section 6 relates GF to dependency parsing, in particular showing the close correspondence between RGL and Universal Dependencies (UD).
• Section 7 summarizes recent work where GF is related to other approaches: WordNet, FrameNet, Construction Grammar, and Abstract Meaning Representation (AMR).
• Section 8 concludes and outlines suggestions for future work. Table 1 shows how Sections 2–7 depend on each other.
2. The Background and Evolution of GF
Grammatical Framework (GF) was born at Xerox Research Centre Europe in 1998 within the project Multilingual Document Authoring (Dymetman, Lux, and Ranta 2000; Ma ̈enpa ̈a ̈ and Ranta 1999). The target was Controlled Natural Language (CNL): pre- cisely defined language fragments for specific domains. The first domains addressed were pharmaceutics, mathematics, and tourist phrasebooks.
GF was expected to enable high-quality translation via semantic interlinguas. The semantic interlingua would define the meaning to be preserved in translation. The translation itself would be done by parsing the source language into the interlingua, and generating the target language from it. Such systems had been built before GF, but they were considered expensive to develop and maintain. Rosetta (1994) was perhaps the most comprehensive system of this kind, using semantic structures inspired by Montague grammar (Montague 1974) as interlingua.
The mission of GF was to make it cheaper to build multilingual CNL systems. This was to be achieved by developing a declarative, high-level formalism, from which the actual processing components (parsing, generation, interactive editor) could be derived automatically. The overall design of GF software was inspired by another system previously developed at Xerox: the Xerox Finite State Tool (XFST; Beesley and Karttunen 2003). The common features include
• a high-level programming language for grammar writers,
• a low-level but efficient run-time format,
• automatic compilation from high to low level,
• mathematical theory underlying compilation and run-time algorithms,
• neutrality with respect to linguistic theories, and
• development tools for grammar engineering.
In XFST, the high-level language is regular expressions, extended with abstraction mechanisms such as user-definable macros and built-in custom operators. The compiler converts this language to finite state transducers, for which there are portable run-time systems that can be embedded in NLP pipelines.2
GF needed more expressive power than finite state transducers, but the same design principles were followed to make the formalism portable, adaptable, and supported by a mathematical metatheory. The starting point was to build GF on top of a Logical Framework (LF; Harper, Honsell, and Plotkin 1993), based on the Constructive Type TheoryofMartin-Lo ̈f(1984).TheLForiginalsoexplainsthename:
GF = LF + grammar rules
LF had a purpose analogous to GF: declarative definition of different logics, such
as classical, constructive, temporal, modal, dynamic, and so forth. In computerized
mathematics and in software verification, it is often better to work with such special- purpose logics than with one monolithic logic. Implementing them became much more feasible as a generic framework provided algorithms for inference, proof checking, and proof search, together with a notation supporting declarative definitions of logics.
The price to pay for a uniform treatment of logics in LF was that the notation was likewise uniform. The basic framework used prefix notation and ASCII identifiers for all logical operators. For instance, the conjunction of A and B is expressed
(Conj A B)
similarly to the LISP programming language. Later versions of LF introduced “pretty- printing rules” to convert such formulas to more user-friendly notations—for instance,
       (Conj X Y) ==> X & Y
The grammar rule notation of GF arose as a generalization of such pretty-printing rules, adding mechanisms that enable the definition of natural language structures.
In GF, the LF part was to be used for interlinguas, whereas grammar rules would define relations between interlinguas and natural languages (English, French, Finnish, etc.). The grammar rules would be powerful enough to define formal languages as well, such as logics and query languages; one important application area of GF has in- deed been translations between formal and natural languages (Johannisson 2005; Ranta 2011b). The interlingua/LF part in GF was called abstract syntax, and the language parts concrete syntaxes, following familiar compiler terminology. A GF grammar con- sists of one abstract syntax and any number of concrete syntaxes, and is thus inherently a multilingual grammar.
The abstract syntax can be vastly different in domains ranging from mathematics to tourist phrasebooks. However, the linguistic mechanisms needed in the concrete syntax—morphology, agreement, word order—are largely the same in all areas of dis- course. It would be tedious to implement these mechanisms over and over again in different applications. The solution to this problem was to develop a library of basic linguistic structures. The library, called the Resource Grammar Library (RGL), started to develop in 2001. It was inspired primarily by the Core Language Engine (Alshawi 1992), where a similar need had been identified. It is also related to Xerox ParGram (Butt et al. 2002) and the LiNGO Matrix (Bender and Flickinger 2005), which are collections of grammars for different languages.3
The RGL implements morphology and syntax rather than semantics. It was not meant to be a translation interlingua in itself, but to boost the development of domain- specific semantic CNLs by taking care of linguistic details. Not only does it save the work of implementing morphology and syntax again and again. It also has its own common abstract syntax, which provides a common Application Programming Inter- face (API) enabling a CNL implementation for many languages simultaneously, sharing almost all of the source code except content words (see Section 5).
But does GF scale up to mainstream NLP tasks? Its original mission is expressed in the diagram of the coverage/precision trade-off (Figure 4). Although it seems im- possible to achieve complete coverage and complete precision at the same time, it is possible to opt for either of them and make progress by improving the other. Much of NLP opts for 100% coverage, but tries to improve precision, as measured, for instance, by the BLEU score in translation (Papineni et al. 2002). The state of the art in machine translation achieves around 50% or less, depending on language pair, as shown by the latest Workshop on Machine Translation (WMT) evaluations.4 The philosophy of GF has been orthogonal to this: Maintain full precision, and develop by increasing coverage.
Figure 4 relates the coverage–precision trade-off to typical NLP tasks. High cover- age is needed in “consumer” tasks, where the input is unpredictable. The system must be able to cope with millions of “concepts,” such as translation units (e.g., words and multiwords). High precision, in contrast, is needed in “producer” tasks, such as the publication of some content in different languages. Human translation is still the state of the art in such tasks, but automation can be possible if the type of expected content canbefixed.TheME ́TE ́Osystem(Chandioux1976)isanearlyexampleofthis.Atypical modern example is e-commerce Web sites selling to multiple countries. The size of such tasks is typically just thousands of concepts, rather than millions.5
Producer and consumer tasks are known as dissemination and assimilation, re- spectively, in machine translation terminology (Hutchins and Somers 1992). The distinc- tion has gone roughly parallel to the use of symbolic vs. statistical methods. But early MT systems often used symbolic methods for assimilation as well, and this tradition continues in Apertium (Forcada et al. 2011). The advantages of symbolic methods include
• explainability: the system can show why it yields a certain result, by for instance showing a syntax tree;
• programmability: it is possible to fix bugs without breaking everything else and
• data austerity: no large amounts of data are needed. Data austerity is particularly important for low-resource languages, which may be un- likely ever to have enough data for statistical methods. Writing linguistic rules can then be a way out. Another aspect is the possibility to use additional knowledge: Statistical methods typically learn translation from parallel texts, whereas correct translation can also require knowledge that is impossible to extract from the texts alone (Kay 2017).
It is natural to ask whether GF could be used for wide-coverage tasks by sacrificing some precision but maintaining explainability, programmability, and data austerity. This question has been the focus of much of GF research for the last ten years. It was firstenabledbyefficientandscalableparsingtechniques(Ljunglo ̈f2004;Angelov2009; Angelov and Ljunglo ̈f 2014). The first experiments addressed full-scale morphology implementations (Forsberg and Ranta 2004; De ́trez and Ranta 2012), hybrid GF-SMT translation (Enache et al. 2012), and multilingual lexicon extraction (Virk et al. 2014; Angelov 2014). Much of the focus has been on machine translation, with an early mobile demo (Angelov, Bringert, and Ranta 2014) and an emphasis on explainability rather than optimized BLEU scores. The prospects for Explainable Machine Translation (XMT) are studied in Ranta (2017), and the general name for all the approaches described in this paper could be Explainable NLP.
Figure 5 shows the architecture of a GF-based explainable MT system as an instance of Explainable Artificial Intelligence: an AI system which, in addition to the output, also gives an explanation of why this output is produced: For instance, that an image is recognized as a cat because it has a tail and whiskers and four legs (Gunning 2017). The user can inspect the explanation to judge whether the output is correct or plausible. Such explanations are clearly desirable in MT, where the user typically knows just one of the languages and cannot judge correctness when only seeing the input and output. The architecture shown in Figure 5 is inspired by the notion of certifying algorithms (McConnell et al. 2011), where the explanation, a certificate, can be inspected by an independent checker program. In the case of GF-based XMT, the certificate is the interlingual representation (abstract syntax tree), which specifies the meaning that has been extracted from the source utterance by the analyzer and rendered in the target language. This tree can be inspected independently by the user, and also translated
back to the source language by the grammar (“linearization”). Because of this, the tree could actually be produced by a black box such as a neural parser, and still be used with confidence.
3. Modeling Languages with Abstract Syntax
Abstract syntax is a tree representation used in compilers and in programming lan- guage semantics. Its purpose is to abstract away from details considered irrelevant for semantics:
• the shape of tokens, for example, if one uses != or <> as inequality symbol;
• the order of constituents, for example, if one uses infix (2 + 3), postfix (2 3 +) or prefix (+ 2 3) notation; and
• the number of tokens, for example, if one includes the keyword then in “if-then-else” statements or not.
Putting these abstractions together makes it possible to build common abstract syntax representations for languages that look totally different. A typical example is the transla- tion of high-level programming languages to machine languages in a compiler. Figure 6 shows an abstract syntax tree for an arithmetic expression in Java and its translation to Java Virtual Machine assembly language, illustrating variation in the shape, order, and number of tokens.
The place of abstract syntax in a compiler is intermediate between the front end and the back end. A chief component in the front end is parsing, which converts the input source code (string of tokens) to an abstract syntax tree (AST). The back end applies the opposite procedure of linearization to convert the AST into a string in the target language. The back end can also involve semantic analysis such as mapping the variable symbols into memory addresses or registers, or perform optimizations on the code, for instance to minimize the size of the target code or its memory usage. Such back-end operations are implemented by syntax-directed translation, which means recursive functions operating on ASTs (Aho et al. 2006).