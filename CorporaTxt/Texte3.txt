LESSLEX: Linking Multilingual Embeddings to SenSe Representations of LEXical Items
We present LESSLEX, a novel multilingual lexical resource. Different from the vast majority of existing approaches, we ground our embeddings on a sense inventory made available from the BabelNet semantic network. In this setting, multilingual access is governed by the mapping of terms onto their underlying sense descriptions, such that all vectors co-exist in the same semantic space. As a result, for each term we have thus the ”blended” terminological vector along with those describing all senses associated to that term. LESSLEX has been tested on three tasks relevant to lexical semantics: conceptual similarity, contextual similarity, and semantic text similarity. We experimented over the principal data sets for such tasks in their multilingual and crosslingual variants, improving on or closely approaching state-of-the-art results. We conclude by arguing that LESSLEX vectors may be relevant for practical applications and for research on conceptual and lexical access and competence.
1. Introduction
In the last decade, word embeddings have received growing attention. Thanks to their strength in describing, in a compact and precise way, lexical meaning (paired with a tremendous ease of use), word embeddings conquered a central position in the lexical semantics stage. Thanks to the speed and intensity of their diffusion, the impact of deep architectures and word embeddings has been compared to a tsunami hitting the NLP community and its major conferences (Manning 2015). Word embeddings have been successfully applied to a broad—and still growing—set of diverse application fields,
such as computing the similarity between short texts (Kenter and De Rijke 2015), full documents (Kusner et al. 2015), or both (Le and Mikolov 2014). Also, by looking at traditional NLP such as parsing, embeddings proved to be an effective instrument for syntactical parsing—both dependency (Hisamoto, Duh, and Matsumoto 2013; Bansal, Gimpel, and Livescu 2014) and constituency parsing (Andreas and Klein 2014)—and semantic parsing as well (Berant and Liang 2014).
Within this phenomenon, multilingual and crosslingual word embeddings have gained a special status, thanks to the strong and partly unanswered pressure for devis- ing tools and systems to deal with more than one language at a time. Among the main areas where multilingual and crosslingual resources and approaches are solicited, there are of course machine translation (Cho et al. 2014; Luong, Pham, and Manning 2015), crosslingual document categorization (Kocˇisky`, Hermann, and Blunsom 2014; Gouws, Bengio, and Corrado 2015), and sentiment analysis (Tang et al. 2014).
Consistently with the assumption that word semantics is a function of the context (such that words occurring in similar context tend to deliver similar meanings [Harris 1954]), research on word embeddings mostly focused on providing descriptions for terms rather than for word senses, by often disregarding the issue of lexical ambiguity. This fact has historically led (with some exceptions, reviewed hereafter) to a separate growth of research aimed at building word embeddings from that rooted in lexico- graphic resources (in the tradition of WordNet [Miller 1995] and Babelnet [Navigli and Ponzetto 2010, 2012]) and aimed at developing cognitively plausible approaches to lex- ical meaning and to the construction of lexical resources. These approaches distinguish between word meanings (senses) and word forms (terms). The basic unit of meaning is the synset, a set of synonyms that provide (possibly multilingual) lexicalizations to the represented sense, like a lexical dress. Synsets overall describe a semantic network whose nodes are word meanings, linked by semantic relations (such as hypernymy, hyponymy, meronymy, holonymy, etc.). This kind of approach is far in essence from any kind of distributional hypothesis, in that it never happens that a synset conflates two senses.1 Conversely, the word embeddings for a term provide a synthetic description capturing all senses possibly associated with that term. Word senses have been tradi- tionally used to perform tasks such as word sense disambiguation (WSD) or word sense induction (WSI): individuating which senses occur in a given text may be a precious cue for categorizing documents, to extract meaningful terms, the list of concepts employed along with their mutual relations, etc. The shift of paradigms from lexicographic to distributional approaches has gone hand in hand with the rise of new tasks: besides WSD, semantic similarity (between terms, sentences, paragraphs, whole documents) has emerged as a new, vibrating task in the NLP community: A task perfectly fitting to the geometric descriptions delivered through vectors of real numbers over a continuous, high-dimensional Euclidean space.
However, despite impressive results obtained by systems using word embeddings, some issues were largely left unexplored, such as (i) the links between representations delivered through word embeddings vs. lexicographic meaning representations; (ii) the cognitive plausibility of the word embeddings (which is different from testing the agreement with conceptual similarity ratings); and (iii) the ways to acquire word em- beddings to deliver common-sense usage of language (more on common-sense knowl- edge later on). In particular, different from lexicographic resources where the minimal addressable unit of meaning is word sense (the synset), with few notable exceptions (such as NASARI [Camacho-Collados, Pilehvar, and Navigli 2015b] and SENSEEMBED [Iacobacci, Pilehvar, and Navigli 2015]), word embeddings typically describe terms. This means that different (though close) vectorial descriptions are collected for terms such as table, board, desk for each considered language; whereas in a resource based on senses just one description for the sense of table (e.g., intended as “a piece of furniture having a smooth flat top that is usually supported by one or more vertical legs”) would suffice. Of course this fact has consequences on the number of vectors involved in multilingual and cross-language applications: One vector per term per language in the case of terminological vectors, one per sense—regardless of the language—otherwise.
One major challenge in the lexical semantics field is, to date, that of dealing with as many as possible languages at the same time (e.g., BabelNet covers 284 different languages),2 so to enable truly multilingual and crosslingual applications. In this work we propose LESSLEX, a novel set of embeddings containing descriptions for senses rather than for terms. The whole approach stems from the hypothesis that to deal with multilingual applications, and even more in crosslingual ones, systems can benefit from compact, concept-based representations. Additionally, anchoring lexical representations to senses should be beneficial in providing more precise and to some extent more under- standable tools for building applications. The evaluation of our vectors seems to support such hypotheses: LESSLEX vectors have been tested in a widely varied experimental setting, providing performances at least on par with state-of-the-art embeddings, and sometimes substantially improving on these.
2. Related Work
Many efforts have been invested in the last decade in multilingual embeddings; a recent and complete compendium is provided by Ruder, Vulic ́, and Søgaard (2019). In general, acquiring word embeddings amounts to learning some mapping between bilingual resources, so to induce a shared space where words from both languages are represented in a uniform language-independent manner, “such that similar words (regardless of the actual language) have similar representations” (Vulic ́ and Korhonen 2016, page 247). A partially different and possibly complementary approach that may be undertaken is sense-oriented; it is best described as a graph-based approach, and proceeds by exploiting the information available in semantic networks such as WordNet and BabelNet.
2.1 Multilingual Embedding Induction
With regard to the first line of research, in most cases the alignment between two languages is obtained through parallel data, from which as close as possible vectorial descriptions are induced for similar words (see, e.g., the work by Luong, Pham, and Manning [2015]). A related approach consists in trying to obtain translations at the sentence level rather than at the word level, without utilizing word alignments (Chandar et al. 2014); the drawback is, of course, that large parallel corpora are re- quired, which may be a too restrictive constraint on languages for which only scarce resources are available. In some cases (pseudo-bilingual training), Wikipedia has thus been used as a repository of text documents that are circa aligned (Vulic ́ and Moens 2015). Alternatively, dictionaries have been used to overcome the mentioned limitations, by translating the corpus into another language (Duong et al. 2016). Dictionaries have been used as seed lexicons of frequent terms to combine language models acquired separately over different languages (Mikolov, Le, and Sutskever 2013; Faruqui and Dyer 2014). Artetxe, Labaka, and Agirre (2018) propose a method using a dictionary to learn an embedding mapping, which in turn is used to iteratively induce a new dictionary in a self-learning framework by starting from surprisingly small seed dictionaries (a parallel vocabulary of aligned digits), that is used to iteratively align embedding spaces with performances comparable to those of systems based on much richer resources. A different approach consists in the joint training of multilingual models from parallel corpora (Gouws, Bengio, and Corrado 2015; Coulmance et al. 2015).
Also sequence-to-sequence encoder-decoder architectures have been devised, to train systems on parallel corpora with the specific aim of news translation (Hassan et al. 2018). Multilingual embeddings have been devised to learn joint fixed-size sentence representations, possibly scaling up to many languages and large corpora (Schwenk and Douze 2017). Furthermore, pairwise joint embeddings (whose pairs usually involve the English language) have been explored, also for machine translation, based on dual- encoder architectures (Guo et al. 2018).
Conneau et al. (2018) propose a strategy to build bilingual dictionaries with no need for parallel data (MUSE), by aligning monolingual embedding spaces: This method uses monolingual corpora (for source and target language involved in the translation), and trains a discriminator to discriminate between target and aligned source embeddings; the mapping is trained through the adversarial learning framework, which is aimed at acquiring a mapping between the two sets such that translations are close in a shared semantic space. In the second step a synthetic dictionary is extracted from the resulting shared embedding space. The notion of shared semantic space is relevant to our work, which is, however, concerned with conceptual representations. One main difference with our work is that in our setting the sense inventory is available in ad- vance, and senses (accessed through identifiers that can be retrieved by simply querying BabelNet) are part of a semantic network, and independent from any specific training corpus.
For the present work it is important to focus on ConceptNet Numberbatch (CNN hereafter) (Speer and Chin 2016; Speer, Chin, and Havasi 2017). CNN has been built through an ensemble method combining the embeddings produced by GloVe (Pennington, Socher, and Manning 2014) and Word2vec (Mikolov et al. 2013) with the structured knowledge from the semantic networks ConceptNet (Havasi, Speer, and Alonso 2007, Speer and Havasi 2012) and PPDB (Ganitkevitch, Van Durme, and Callison-Burch 2013). CNN builds on ConceptNet, whose nodes are compound words such as ”go-to-school.” ConceptNet was born with a twofold aim: at expressing concepts “which are words and phrases that can be extracted from natural language text,” and assertions “of the ways that these concepts relate to each other” (Speer and Havasi 2012). Assertions have the form of triples where concept pairs are related by a set of binary relations:3 Importantly enough, this knowledge base grasps common sense, which is typically hard to acquire by artificial systems. We refer to common sense as a portion of knowledge that is both widely accessible and elementary (Minsky 1975), and reflecting typicality traits encoded as prototypical knowledge (Rosch 1975). This sort of knowledge is about “taking for granted” information, a set of “obvious things people normally know and usually leave unstated” (Cambria et al. 2010, page 15). To the best of our knowledge, no previous system for learning word embeddings has explicitly focused on the acquisition of this sort of knowledge; by contrast, ConceptNet is at the base of other projects concerned with the development of lexical resources (Mensa, Radicioni, and Lieto 2018) and their usage along with formal ontologies (Lieto, Radicioni, and Rho 2015, 2017).
However, ConceptNet is principally a lexical resource, and as such it disregards the conceptual anchoring issue: If we consider the term bat, the bat node in ConceptNet mixes all possible senses for the given term, such as the nocturnal mammal, the implement used for hitting the ball, the acronym for “brown adipose tissue,” an entity such as the radar-guided glide bomb used by the US Navy in World War II, and so forth.4 The lack of conceptual anchoring is also a main trait in CNN, as for most word embeddings: Vectors typically flatten all senses, by reflecting their distribution over some corpus approximating human language, or fractions of it.
2.2 Sense Embeddings: Multi-Prototype, Sense-Oriented Embeddings
Some work on word embeddings have dealt with the issue of providing different vecto- rial descriptions for as many senses associated with a given term. Such approaches stem from the fact that typical word embeddings mostly suffer from the so-called meaning conflation deficiency, which arises from representing all possible meanings of a word as a single vector of word embeddings. The deficiency consists of the “inability to discriminate among different meanings of a word” (Camacho-Collados and Pilehvar 2018, page 743).
In order to account for lexical ambiguity, Reisinger and Mooney (2010) propose representing terms as collections of prototype vectors; the contexts of a term are then partitioned to construct a prototype for the sense in each cluster. In particular, for each word different prototypes are induced, by clustering feature vectors acquired for each sense of the considered word. This approach is definitely relevant to ours for the attempt at building vectors to describe word senses rather than terms. However, one main difference is that the number of sense clusters K in our case is not a parameter (admit- tedly risking to inject noisy clusters as K grows), but it relies on the sense inventory of BabelNet, which is periodically updated and improved. The language model proposed by Huang et al. (2012) exploits both local and global context that are acquired through a joint training objective. In particular, word representations are computed while learning to discriminate the next word, given a local context composed of a short sequence of words, and a global context composed of the whole document where the word sequence occurs. Then, the collected context representations are clustered, and each occurrence of the word is labeled with its cluster, and used to train the representation for that cluster. The different meaning groups are thus used to learn multi-prototype vectors,
in the same spirit as in the work by Reisinger and Mooney (2010). Also relevant to our present concerns, the work by Neelakantan et al. (2014) proposes an extension to the Skip-gram model to efficiently learn multiple embeddings per word type: Interestingly enough, this approach obtained state-of-the-art results in the word similarity task. The work carried out by Chen et al. (2015) directly builds on a variant of the Multi-Sense Skip-Gram (MSSG) model by Neelakantan et al. (2014) for context clustering purposes. Namely, the authors propose an approach for learning word embeddings that relies on WordNet glosses composition and context clustering; this model achieved state-of-the- art results in the word similarity task, improving on previous results obtained by Huang et al. (2012) and by Chen, Liu, and Sun (2014).
Another project we need to mention is NASARI. In the same spirit as BabelNet, NASARI puts together two sorts of knowledge: one available in WordNet, handcrafted by human experts, based on synsets and their semantic relations, and one available in Wikipedia, which is the outcome of a large collaborative effort. Pages in Wikipedia are considered as concepts. The algorithm devised to build NASARI consists of two main steps: For each concept, all related Wikipedia pages are collected by exploiting Wikipedia browsing structure and WordNet relations. Then, vectorial descriptions are extracted from the set of related pages. The resource was initially delivered with vectors describing two different semantic spaces: lexical (each sense was described through lexical items) and unified (each sense was described via synset identifiers). In both cases, vector features are terms/senses that are weighted and sorted based on their semantic proximity to the concept being represented by the current vector (Camacho-Collados, Pilehvar, and Navigli 2015b). In subsequent work NASARI has been extended through the introduction of a distributional description: In NASARI embeddings each item (concept or named entity) is defined through a dense vector over a 300-dimensions space (Pilehvar and Navigli 2015). NASARI vectors have been acquired by starting from the vectors trained over the Google News data set, provided along with the Word2vec toolkit. All the NASARI vectors also share the same semantic space with Word2vec, so that their representations can be used to compute semantic distances be- tween any two such vectors. Thanks to the structure provided by the BabelNet resource, the resulting 2.9M embeddings are part of a huge semantic network. Unless differently specified, in the rest of this work we will refer to the embedded version of NASARI, which is structurally more similar to our resource. NASARI includes sense descriptions for nouns, but not for other grammatical categories.
Another resource that is worth mentioning is SENSEEMBED (Iacobacci, Pilehvar, and Navigli 2015); the authors propose here an approach for obtaining continuous representations of individual senses. In order to build sense representations, the au- thors exploited Babelfy (Moro, Raganato, and Navigli 2014) as a WSD system on the September-2014 dump of the English Wikipedia.5 Subsequently, the Word2vec toolkit has been used to build vectors for 2.5 millions of unique word senses.
2.3 Contextualized Models
Although not originally concerned with multilingual issues, a mention to works on contextualized embeddings is due, given their large diffusion. Such models are de- vised to learn dynamic word embeddings representations. Two main strategies can be outlined (Devlin et al. 2019), that apply pre-trained language models to downstream tasks: feature-based and fine-tuning. In the former case, task-specific architectures are used as additional features (like in the case of ELMo [Peters et al. 2018]). Approaches of this sort have been extended to account for sentence (Logeswaran and Lee 2018) and paragraph (Le and Mikolov 2014) embeddings. Peters et al. (2018) extend traditional embeddings by extracting context sensitive features. This kind of model is aimed at grasping complex (such as syntactic and semantic) features associated with word usage, and also to learn how these features vary across linguistic contexts, like in modeling polysemy. ELMo embeddings encode the internal states of a language model based on an LSTM. In the latter case—that is, fine-tuning approaches—minimal task-specific parameters are utilized, and are trained on supervised downstream tasks to tune pre- trained parameters, as in the case of OpenAI GPT (Radford et al. 2019). Unsupervised pre-training approaches are in general known to benefit from nearly unlimited amounts of available data, but approaches exist also showing effective transfer from supervised tasks with large data sets, for example, in sentence representation from NL inference data (Conneau et al. 2017). Specifically, in this work it is shown how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference data sets outperform unsupervised approaches like that by Kiros et al. (2015), and that natural language inference is appropriate for transfer learning to further NLP tasks. BERT vectors (Devlin et al. 2019) rely on other embedding representations, with the notable difference that they model bidirectional context, different from a model such as ELMo, which uses a concatenation of independently trained left-to-right and right- to-left language models.
Until recently, contextualized embeddings of words such as, for example, ELMo and BERT, obtained outstanding performance in monolingual settings, but they seemed to be less suited for multilingual tasks. Aligning contextual embeddings is challenging, because of their dynamic nature. For example, word embeddings tend to be consistent across language variations (Aldarmaki, Mohan, and Diab 2018), whereas multilingual vector spaces have more difficulty in representing individual words (such as, e.g., homographs with unrelated senses and phrasal verbs) because of their different usage distributions. As a result, using such words in the alignment dictionary may under- mine the mapping (Aldarmaki and Diab 2019). Another sort of difficulty that may be experienced by contextualized models is represented by cases where a single word of a morphologically complex language corresponds to several words of a morphologically simpler language: In such cases, having a vector for each word might not be appropriate to grasp their meanings across languages (Artetxe and Schwenk 2019, page 3).
However, recent work has been carried out that uses contextual word embeddings for multilingual transfer. The work by Schuster et al. (2019) is reportedly related to MUSE (Conneau et al. 2018): However, different from that approach, aimed at align- ing embeddings at the token level, this approach produces alignments for contextual embeddings in such a way that context-independent variants of the original monolin- gual spaces are built, and their mapping is used to acquire an alignment for context- dependent spaces. More specifically, context-independent embedding anchors are used to learn an alignment that can then be used to map the original spaces with contex- tual embeddings. With regard to the handling of polysemy, the embeddings obtained through the described approach reflect the multiple senses assumed by the word in dif- ferent contexts. An alignment based on words in same context, using parallel sentences, is proposed by Aldarmaki and Diab (2019).
3. LESSLEX Generation
The generation of LESSLEX relies on two resources: BabelNet and CNN. We briefly describe them for the sake of self-containedness. BabelNet is a wide-coverage mul- tilingual semantic network resulting from the integration of lexicographic and ency- clopedic knowledge from WordNet and Wikipedia. Word senses are represented as synsets, which are uniquely identified by Babel Synset identifiers (e.g., bn:03739345n). Each synset is enriched by further information about that sense, such as its possible lexicalizations in a variety of languages, its gloss (a brief description), and its Wikipedia Page Title. Moreover, it is possible to query BabelNet to retrieve all the meanings (synsets) for a given term. Although the construction of BabelNet is by design essential to our approach, in principle we could plug in different sets of word embeddings. We chose CNN word embeddings as our starting point for a number of reasons, namely: its vectors are to date highly accurate; all such vectors are mapped onto a single shared multilingual semantic space spanning over 78 different languages; it ensures reasonable coverage for general purposes use (Speer and Lowry-Duda 2017); it allows dealing in a uniform way with multiword expressions, compound words (Havasi, Speer, and Alonso 2007), and even flexed forms; and it is released under the permissive MIT License.
The algorithm for the generation of LESSLEX is based on an intuitive idea: to ex- ploit multilingual terminological representations in order to build precise and punctual conceptual representations. Without loss of generality, we introduce our methodology by referring to nominal senses, although the whole procedure also applies to verb and adjectival senses, so that in the following we will switch between sense and concept as appropriate. Each concept in LESSLEX is represented by a vector generated by averaging a set of CNN vectors. Given the concept c, we retrieve it in BabelNet to obtain the sets {T l1 (c), . . . , T ln (c)} where each T l (c) is the set of lexicalizations in the language l for c.6 We then try to extract further terms from the concepts’ English gloss and English Wikipedia Page Title (WT from now on). The final result is the set T +(c) that merges all the multilingual terms in each T l(c) plus the terms extracted from the English gloss and WT. In T +(c) we retain only those terms that can be actually found in CNN, so that the LESSLEX vector ⃗c can be finally computed by averaging all the CNN vectors associated to the terms in T +(c).
3.1 Selecting the Sense Inventory: Seed Terms
Because the generation algorithm creates a representation for conceptual elements (be they nominal, verbal, or adjectival senses), it is required to define which concepts will be hosted in the final resource. For this purpose we define a set of terms that we call seed terms. Seed terms are taken from different languages and different POS (nouns, verbs, and adjectives are presently considered), and their meanings (retrieved via BabelNet) constitute the set of senses described by LESSLEX vectors. Because of the polysemy of language and because the seed terms are multilingual, different seed terms can retrieve the same meaning. Seed terms do not affect the generation of a vector, but they rather determine the coverage of LESSLEX, since they are used to acquire the set of concepts 3.2 Extending the Set of Terms
As anticipated, we not only rely on the lexicalizations of a concept to build its T +, but we also try to include further specific words, parsed from its English gloss and WT. The motivation behind this extension is the fact that we want to prevent T + from containing only one element: In such a case, the vector for the considered sense would coincide with that of the more general term, possibly conflating different senses. In other words, enriching T + with further terms is necessary to reshape vectors that have only one associated term as lexicalization. For instance, starting from the term sunseteng we encounter the sense bn:08410678n (representing the city of Sunset, Texas). This sense is provided with the following lexicalizations:
T eng = {sunseteng}; T spa = {sunsetspa}; T fra = {sunsetfra}.
However, out of these three terms only sunseteng actually appears in CNN, giving us a final singleton T + = {sunseteng}. At this point no average can be performed, and the final vector in LESSLEX for this concept would be identical to the vector of sunseteng in CNN. Instead, if we take into consideration the gloss ‘’Township in Starr County, Texas,‘’ we can extract townshipeng and append it in T +, thus obtaining a richer vector for this specific sense of sunset. In the following sections we describe the two strategies that we developed in order to extract terms from WTs and glosses. The extension strategies are applied for every concept, but in any case, if the final T + contains a single term (|T +| = 1), then we discard the sense and we do not include its vector in LESSLEX.
3.2.1 Extension Via Wikipedia Page Title. The extension via WT only applies to nouns, because senses for different POSs are not present in Wikipedia. In detail, if the concept has a Wikipedia Page attached and if the WT provides a disambiguation or specifica- tion (e.g., Chips (company) or Magma, Arizona) we extract the relevant component (by exploiting commas and parentheses of the Wikipedia naming convention) and search for it in CNN. If the whole string cannot be found, we repeat this process by removing the leftmost word of the string until we find a match. In so doing, we search for the maximal substring of the WT that has a description in CNN. This allows us to obtain the most specific and yet defined term in CNN. For instance, for the WT Bat (guided bomb) we may not have a match in CNN for guided bomb, but we can at least add bomb to the set of terms in T +.
3.2.2 Extension Via Gloss. Glosses often contain precious pieces of information that can be helpful in the augmentation of the terms associated with a concept. We parse the gloss and extract its components. By construction, descriptions provided in BabelNet glosses can originate from either WordNet or Wikipedia (Navigli and Ponzetto 2012). In the first case we have (often elliptical) sentences, such as (bn:00028247n – door) “a swinging or sliding barrier that will close the entrance to a room or building or vehicle.” On the other side, Wikipedia typically provides a plain description like “A door is a panel that makes an opening in a building, room or vehicle.” Thanks to the regularity of these languages, with few regular expressions on POS patterns7 we are able to collect enough information to enrich T +. We devised several rules according to each sense POS; the complete list is reported in Table 1. As an example, from the following glosses we extract the terms in bold (the matching rule is shown in square brackets):
- [Noun-2] bn:00012741n (Branch) A stream or river connected to a larger one.
- [Noun-3] bn:00079944n (Winner) The contestant who wins the contest.
- [Noun-1] bn:01276497n (Plane (river)) The Plane is a river in Brandenburg, Germany, left tributary of the Havel.
- [Verb-2] bn:00094850v (Tee) Connect with a tee.
- [Verb-3] bn:00084198v (Build) Make by combining materials and parts.
In Figure 2 we provide an example of the generation process for three concepts, provided by the seed terms gateeng and gateita. For the sake of simplicity, we only show the details regarding two languages (English and Italian). Step (1) shows the input terms. In step (2) we retrieve three meanings for gateeng and one for gateita, which has already been fetched because it is also a meaning for gateeng. For each concept we collect the set of lexicalizations in all considered languages, plus the extensions extracted from WT and gloss. We then merge all such terms in T +, by retaining only those that can be actually found in CNN. Once the T + sets are computed, we access CNN to retrieve the required vectors for each set (3) and then we average them, finally obtaining the vectors for the concepts at hand (4).
3.3 LESSLEX Features
We now describe the main features of LESSLEX, together with the algorithm to compute conceptual similarity on this resource. The final space in which LESSLEX vectors reside is an extension of the CNN multilingual semantic space. Each original CNN vector coexists with the set of vectors that represent its underlying meanings. This peculiar feature allows us to compute the distance between a term and each of its corresponding senses, and such distance is helpful to determine, given a pair of terms, in which sense they are intended. For example, in assessing the similarity of two terms such as “glass” and “eye,” most probably the recalled senses would differ from those recalled for the pairs “glass” and “window,” and “glass,” “wine.”
3.3.1 LessLex Building. The LESSLEX resource8 has been generated from a group of seed terms collected by starting from 56, 322 words taken from the Corpus of Contempo- rary American English (COCA) (Davies 2009),9 19, 789 terms fetched from the relevant dictionaries of the Internet Dictionary Project,10 and the 12,544 terms that appear in the data sets that we used during the evaluation. All terms were POS tagged and
duplicates removed beforehand. The final figures of the resource and details concerning its generation are reported in Table 2.
We started from a total of 84,620 terms, and for 65,629 of them we were able to retrieve at least one sense in BabelNet. The T + cardinality shows that our vectors were built by averaging about six CNN vectors for each concept. Interestingly, verbs seem to have much richer lexical sets. The final number of senses in LESSLEX amounts to 174,300, with a vast majority of nouns. We can also see an interesting overlap between the group of senses associated with each term. If we take nouns, for example, we have around 42K terms providing 148K unique senses (3.5 per term), while the average polysemy per term counting repetitions amounts to 6.12. So, we can observe that approximately three senses per term are shared with some other term. A large number of concepts are discarded because they only have one term inside T +: These are named entities or concepts with poor lexicalization sets. The extraction process provided a grand total of about 228K terms, and on average each T + contains 1.40 additional terms extracted from WTs and glosses.
Out of the 117K senses in WordNet (version 3.0), roughly 61K of them are covered in LESSLEX. It is, however, important to note that additional LESSLEX vectors can be built upon any set of concepts, provided that they are represented in BabelNet (which contains around 15M senses) and that some of their lexicalizations are covered in CNN (1.5M terms for the considered languages).3.3.2 Computing Word Similarity: Maximization and Ranked-Similarity. The word similarity task consists of computing a numerical score that expresses how similar two given terms are. Vectorial resources such as CNN can be easily utilized to solve this task: In fact, because terms are represented as vectors, the distance (usually computed through cosine similarity, or some other variant of angular distance) between the two vectors associated with the input terms can be leveraged to obtain a similarity score. Although terminological resources can be directly used to compute a similarity score between words, conceptually grounded resources (e.g., NASARI, LESSLEX) do not allow us to directly compute world similarity, but rather conceptual similarity. In fact, such resources are required to determine which senses must be selected while computing the score for the terms. In most cases this issue is solved by computing the similarity between all the combinations of senses for the two input terms, and then by selecting the maximum
similarity as the result score (Pedersen, Banerjee, and Patwardhan 2005). In formulae, given a term pair ⟨t1,t2⟩ and their corresponding list of senses s(t1) and s(t2), the similarity can be computed as
sim(t1,t2) = max  sim(⃗ci,⃗cj)  (1) ⃗ci ∈s(t1 ),⃗cj ∈s(t2 )
where sim(⃗ci,⃗cj) is the computation of conceptual similarity using the vector represen- tation for the concepts at hand.
To compute the conceptual similarity between LESSLEX vectors we have devised a different approach, which we call ranked similarity. Because we are able to determine not only the distance between each two senses of the input terms, but also the distance between each input term and all of its senses, we use this information to fine tune the computed similarity scores and use ranking as a criterion to grade senses’ relevance. In particular, we hypothesize that the relevance of senses for a given term can be helpful for the computation of similarity scores, so we devised a measure that also accounts for the ranking of distances between senses and seed term. It implements a heuristics aimed at considering two main elements: the relevance of senses (senses closer to the seed term are preferred), and similarity between sense pairs. Namely, the similarity between two terms t1, t2 can be computed as:
rnk-sim(t1,t2) = (2) max   (1 − α) · (rank(⃗ci) + rank(⃗cj))−1  +  α · cos-sim(⃗ci,⃗cj)  ,
⃗ci ∈s(t1 ), ⃗cj ∈s(t2 )
where α is used to tune the balance between ranking factor and raw cosine similar- ity.11 We illustrate the advantages of the ranked similarity with the following example (Figure 3). Let us consider the two terms teacher and student, whose gold-standard similarity score is 0.50.12 One of the senses of teacher is bn:02193088n (The Teacher (1977 film) - a 1977 Cuban drama film) and one of the senses of student is bn:02935389n (Stu- dent (film) - a 2012 Kazakhstani drama film). These two senses have a cosine similarity in LESSLEX of 0.81; such a high score is reasonable, because they are both drama movies. However, it is clear that an annotator would not refer to these two senses for the input terms, but rather to bn:00046958n (teacher - a person whose occupation is teaching) and bn:00029806n (student - a learner who is enrolled in an educational institution). These two senses obtain a similarity score of 0.61, which will not be selected because it is lower than 0.81 (as computed through the formula in Equation (1)). However, if we take into consideration the similarities between the terms teacher and student and their associated senses, we see that the senses that one would select—while requested to provide a similarity score for the pair—are much closer to the seed terms. The proposed measure involves re-ranking the senses based on their proximity to the term representation, thereby emphasizing more relevant terms. We finally obtain similarity of 0.44 for the movie-related senses, whereas the school-related senses pair obtains a similarity of 0.55, which will be selected and better correlates with human rating.
Because the ranked-similarity can be applied only if both terms are available in CNN (so that we can compute the ranks among their senses), we propose a twofold set-up for the usage of LESSLEX. In the first set-up we only make use of the ranked- similarity, so in this setting if at least one given term is not present in CNN we discard the pair as not covered by the resource. In the second set-up (LESSLEX-OOV, designed to deal with Out Of Vocabulary terms) we implemented a fallback strategy to ensure higher coverage: In this case, in order to cope with missing vectors in CNN, we adopt the max-similarity as similarity measure in place of the ranked-similarity.
4. Evaluation
In order to assess the flexibility and quality of our embeddings we carried out a set of experiments involving both intrinsic and extrinsic evaluation. Namely, we considered three different tasks:
1. The Semantic Similarity task, where two terms or—less frequently—senses are compared and systems are asked to provide a numerical score expressing how close they are; the systems’ output is compared to human ratings (Section 4.1);
303
Computational Linguistics Volume 46, Number 2
2. The more recent Contextual Word Similarity task, asking systems to either assess the semantic similarity of terms taken in context (rather than as pairs of terms taken in isolation), or to decide whether a term has the same meaning in different contexts of usage (Section 4.2); and
3. The Semantic Text Similarity task, where pairs of text excerpts are compared to assess their overall similarity, or to judge whether they convey equal meaning or not (Section 4.3).
4.1 Word Similarity Task
In the first experiment we tested LESSLEX vectors on the word similarity task: Linguistic items are processed in order to compute their similarity, which is then compared against human similarity judgment. Word similarity is mostly thought of as closeness over some metric space, and usually computed through cosine similarity, although different approaches exist, for example, based on cognitively plausible models (Tversky 1977; Jimenez et al. 2013; Lieto, Mensa, and Radicioni 2016a; Mensa, Radicioni, and Lieto 2017). We chose to evaluate our word embeddings on this task because it is a relevant one, for which many applications can be drawn such as Machine Translation (Lavie and Denkowski 2009), Text Summarization (Mohammad and Hirst 2012), and Information Retrieval (Hliaoutakis et al. 2006). Although this is a popular and relevant task, until recently it has been substantially limited to monolingual data, often in English. Con- versely, we collected and experimented on all major crosslingual data sets.
4.1.1 Experimental Setting. In this Section we briefly introduce and discuss the selection of data sets adopted for the evaluation.
A pioneering data set is WordSim-353 (Finkelstein et al. 2002); it was built by start- ing from two older sets of word pairs, the RG-65 and MC-30 data sets (Rubenstein and Goodenough 1965; Miller and Charles 1991). These data sets were originally conceived for the English language and compiled by human experts. They were then translated to multilingual and to crosslingual data sets: The RG-65 has been translated into Farsi and Spanish by Camacho-Collados, Pilehvar, and Navigli (2015a), and the WordSim-353 was translated by Leviant and Reichart (2015b) into Italian, German, and Russian through crowdworkers fluent in such languages. Additionally, WordSim-353 was partitioned by individuating the subset of word pairs appropriate for experimenting on similarity judgments rather than on relatedness judgments (Agirre et al. 2009). The SimLex- 999 data set was compiled through crowdsourcing, and includes English word pairs covering different parts of speech, namely, nouns (666 pairs), verbs (222 pairs), and adjectives (111 pairs) (Hill, Reichart, and Korhonen 2015). It has been then translated into German, Italian, and Russian by Leviant and Reichart (2015a). A data set was proposed entirely concerned with English verbs, the SimVerbs-3500 data set (Gerz et al. 2016); similar to SimLex-999, items herein were obtained from the USF free-association database (Nelson, McEvoy, and Schreiber 2004). The SemEval-17 data set was developed by Camacho-Collados et al. (2017); it contains many uncommon entities, like Si-o-seh pol or Mathematical Bridge encompassing both multilingual and crosslingual data. Finally, another data set was recently released by Goikoetxea, Soroa, and Agirre (2018), in the following referred to as the Goikoetxea data set, built by adding further crosslingual versions for the RG-65, WS-WordSim-353, and SimLex-999 data sets.
In our evaluation both multilingual and crosslingual translations have been used. A multilingual data set is one (like RG) where term pairs ⟨x, y⟩ from language i have been translated as ⟨x′, y′⟩ into a different language, such that both x′ and y′ belong to the same language. An example is ⟨casa, chiesa⟩, ⟨house, church⟩, or ⟨maison, e ́glise⟩. Conversely, in a crosslingual setting (like SemEval 2017, Task 2 - crosslingual subtask), x′ is a term from a language different from that of y′, like in the pair ⟨casa, church⟩.
Many issues can afflict any data set, as is largely acknowledged in the literature (Huang et al. 2012; Camacho-Collados, Pilehvar, and Navigli 2015a; Hill, Reichart, and Korhonen 2015; Camacho-Collados et al. 2017). The oldest data sets are too small (on the order of few tens of word pairs) to attain full statistical significance; until recently, typ- ically similarity and relatedness (association) judgments have been conflated, thereby penalizing models concerned with similarity. Additionally, for such data sets the cor- relation between systems’ results and human ratings is higher than human inter-rater agreement. Because human ratings are largely acknowledged as the upper bound to ar- tificial performance in this kind of task, the point has been raised that such data sets are not fully reliable benchmarks to investigate the correlation between human judgment and systems’ output. Furthermore, a tradeoff exists between the size of the data set and the quality of the annotation: Resources acquired through human experts annotation typically are more limited in size, but featured by higher inter-rater agreement (in the order of .80), whereas larger data sets suffer from a lower (often with < .7) agreement among annotators, thus implying overall reduced reliability. We thus decided to test on all main data sets adopted in the literature, to provide the most comprehensive evaluation, widening the experimental base as much as possible. The most recent data sets are in principle more controlled and reliable—SimLex-999, SimVerbs, SemEval- 2017, Goikoetxea—but still we decided to experiment on all of them, because even RG- 65 and WS-Sim 353 have been widely used until recently. All benchmarks used in the experiments are illustrated in Table 3.
The results obtained by using LESSLEX and LESSLEX-OOV are compared with those obtained by utilizing NASARI and CNN, to elaborate on similarities and differences with such resources. Additionally, we report the correlation indices obtained by exper- imenting with other word and sense embeddings that either are trained to perform on specific data sets (JOINTCHYCB by Goikoetxea, Soroa, and Agirre [2018]), or that directly compare to our resource, as containing both term-level and sense-level vector descriptions (SENSEEMBED and NASARI2VEC). Table 4 summarizes the considered resources and the algorithm used to compute the semantic similarity. In these respects, we adopted the following rationale. When testing with resources that allow for a com- bined use of word and sense embeddings we use ranked-similarity13 (as described in Equation (2)); when testing with sense embeddings we adopt the max similarity/closest senses strategy (Resnik 1995; Budanitsky and Hirst 2006; Pilehvar and Navigli 2015) to select senses; when handling word embeddings we make use of the cosine similarity, by borrowing the same approach as illustrated in Camacho-Collados et al. (2017). 